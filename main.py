from documents_processor import *
import streamlit as st
from retrieval_qa import get_answer
from visualization import plot_retrieval_history
from types import SimpleNamespace

# ç³»ç»Ÿåˆå§‹åŒ–
st.set_page_config(page_title="å¤šæ¨¡æ€å­¦æœ¯åˆ†æç³»ç»Ÿ", page_icon="ğŸ“š", layout="wide")
init_session_state()

# ä¾§è¾¹æ 
st.sidebar.title("ğŸ“š æ–‡æ¡£ç®¡ç†ä»“åº“")
uploaded_files = st.sidebar.file_uploader(
    "ä¸Šä¼ æ–‡çŒ®: pdf/txt/docx...",
    type=["pdf", "txt", "docx"],
    accept_multiple_files=True
)

if uploaded_files:
    file_to_process = [f for f in uploaded_files if f.name not in st.session_state.processed_files]

    if file_to_process:
        with st.spinner("æ­£åœ¨å¤„ç†æ–‡æ¡£ä¸­..."):
            vector_db, keyword_retriever = process_documents(file_to_process)

            if not vector_db:
                st.warning("æœªå¤„ç†åˆ°æœ‰æ•ˆæ–‡æ¡£")
            else:
                st.session_state.vector_db = vector_db
                st.session_state.keyword_retriever = keyword_retriever
                st.sidebar.success(f"æˆåŠŸå¤„ç† {len(file_to_process)} ä¸ªæ–°æ–‡æ¡£")

if st.session_state.active_docs:
    st.sidebar.subheader("æ¿€æ´»æ–‡æ¡£")
    active_docs = st.sidebar.multiselect(
        "é€‰æ‹©é‡ç‚¹åˆ†æçš„æ–‡æ¡£",
        options=list(st.session_state.active_docs),
        default=list(st.session_state.active_docs)
    )
    st.session_state.active_docs = set(active_docs)

if st.session_state.doc_summaries:
    st.sidebar.subheader("æ–‡çŒ®ç²¾å½©æ‘˜è¦")
    for doc_name, doc_summary in st.session_state.doc_summaries.items():
        if doc_name in st.session_state.active_docs:
            with st.sidebar.expander(f"ğŸ“ {doc_name}"):
                st.caption(doc_summary)

st.title("ğŸ“ å¤šæ¨¡æ€å­¦æœ¯æ–‡çŒ®åˆ†æé—®ç­”ç³»ç»Ÿ")
st.caption("ä¸Šä¼ æ–‡çŒ®åï¼Œå³å¯è¿›è¡Œè·¨æ–‡æ¡£æ™ºèƒ½é—®ç­”ä¸åˆ†æ...")

tab1, tab2, tab3, tab4 = st.tabs(["é—®ç­”ç³»ç»Ÿ", "é‡ç‚¹æ–‡çŒ®åˆ†æ", "æ£€ç´¢åˆ†æ", "ç³»ç»Ÿä¿¡æ¯"])

with tab1:
    if st.session_state.messages is None:
        st.session_state.messages = []

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

            if message.get("sources"):
                with st.expander("æ¥æºæ–‡çŒ®è¯¦æƒ…"):
                    for i, doc in enumerate(message["sources"]):
                        sources = doc.metadata.get("source_doc", "Unknown Document")
                        page = doc.metadata.get("page", 0) + 1
                        st.markdown(f"**Original {i+1}**: {sources} (Page {page})")

                        st.info(doc.page_content[:500] + ("..." if len(doc.page_content) > 500 else ""))

    if question := st.chat_input():
        st.session_state.messages.append({"role": "user", "content": question})
        with st.chat_message("user"):
            st.markdown(question)

        with st.spinner("æ­£åœ¨åˆ†ææ–‡çŒ®..."):
            answer, sources = get_answer(question)

        st.session_state.messages.append({
            "role": "assistant",
            "content": answer,
            "sources": sources
        })

        with st.chat_message("assistant"):
            st.markdown(answer)

            if sources:
                with st.expander("æ¥æºæ–‡çŒ®è¯¦æƒ…"):
                    for i, doc in enumerate(sources):
                        source = doc.metadata.get("source_doc", "Unknown Document")
                        page = doc.metadata.get("page", 0) + 1
                        st.markdown(f"**Original {i+1}**: {source} (Page {page})")
                        st.info(doc.page_content[:500] + ("..." if len(doc.page_content) > 500 else ""))

with tab2:
    if not st.session_state.processed_files:
        st.info("è¯·å…ˆä¸Šä¼ æ–‡æ¡£")
    else:
        st.subheader("æ–‡æ¡£åˆ†æå·¥å…·")

        col1, col2 = st.columns(2)
        with col1:
            st.markdown("### ğŸ“Š æ–‡æ¡£ç»Ÿè®¡")
            doc_stats = []
            for doc_name in st.session_state.active_docs:
                chunks = []
                vector_data = st.session_state.vector_db.get()
                for i, metadata in enumerate(vector_data["metadatas"]):
                    if metadata.get("source_doc") == doc_name:
                        chunks.append(vector_data["documents"][i])

                if chunks:  # ç¡®ä¿chunksä¸ä¸ºç©º
                    doc_stats.append({
                        "æ–‡æ¡£": doc_name,
                        "æ®µè½æ•°": len(chunks),
                        "å¹³å‡é•¿åº¦": sum(len(c) for c in chunks) // len(chunks)
                    })

            if doc_stats:
                st.dataframe(doc_stats)

        with col2:
            st.markdown("### ğŸ” æ¦‚å¿µåˆ†æ")
            concept = st.text_input("è¯·è¾“å…¥éœ€è¦åˆ†æçš„æ¦‚å¿µ")
            if concept:
                concept_docs = []
                vector_data = st.session_state.vector_db.get()
                for i in range(len(vector_data["documents"])):
                    if vector_data["metadatas"][i].get("source_doc") in st.session_state.active_docs:
                        if concept.lower() in vector_data["documents"][i].lower():
                            doc_obj = SimpleNamespace()
                            doc_obj.page_content = vector_data["documents"][i]
                            doc_obj.metadata = vector_data["metadatas"][i]
                            concept_docs.append(doc_obj)

                if concept_docs:
                    st.success(f"æ‰¾åˆ° {len(concept_docs)} å¤„æåŠ'{concept}'")
                    for doc in concept_docs[:3]:
                        source = doc.metadata.get("source_doc", "æœªçŸ¥æ–‡æ¡£")
                        page = doc.metadata.get("page", 0) + 1
                        st.markdown(f"ğŸ“„ **{source}** (é¡µç : {page})")
                        st.caption(doc.page_content[:300] + "...")
                else:
                    st.warning(f"æœªæ‰¾åˆ°æåŠåˆ°'{concept}'æ¦‚å¿µçš„æ–‡çŒ®å†…å®¹")

with tab3:
    if not st.session_state.retrieval_history:
        st.info("æ£€ç´¢çš„å†å²ä¸ºç©ºï¼Œå¿«å»æ£€ç´¢ä½ çš„å…³é”®è¯å§!")
    else:
        st.subheader("æ£€ç´¢æ•ˆæœåˆ†æ")
        plot_retrieval_history()

        st.markdown("### ğŸ“ˆ æ£€ç´¢ç»Ÿè®¡")
        last_query = st.session_state.retrieval_history[-1]
        st.metric("å‘é‡æ£€ç´¢ç»“æœ:", last_query["vector_results"])
        st.metric("å…³é”®è¯æ£€ç´¢ç»“æœ:", last_query["keyword_results"])
        st.metric("æœ€ç»ˆæ£€ç´¢ç»“æœ:", last_query["final_results"])

        st.markdown("### ğŸ”§ æ£€ç´¢ä¼˜åŒ–å»ºè®®")
        if last_query["vector_results"] == 0:
            st.warning("å‘é‡æ•°æ®åº“æ£€ç´¢æœªè¿”å›ç»“æœï¼Œå»ºè®®å°è¯•ä¸åŒè¡¨è¿°æ–¹å¼æˆ–æ·»åŠ æ›´å¤šæ–‡çŒ®ä¸Šä¸‹æ–‡")
        elif last_query["keyword_results"] > last_query["vector_results"] * 2:
            st.info("å…³é”®è¯æ£€ç´¢æ•ˆæœæ˜æ˜¾ä¼˜äºå‘é‡æ•°æ®åº“æ£€ç´¢ï¼Œå»ºè®®åœ¨é—®é¢˜ä¸­ä½¿ç”¨æ›´å…·ä½“çš„æ¦‚å¿µæˆ–æœ¯è¯­")
        elif last_query["final_results"] < 3:
            st.error("æœ€ç»ˆè¿”å›çš„æ£€ç´¢ç»“æœè¾ƒå°‘ï¼Œè¯·å°è¯•ç®€åŒ–é—®é¢˜æˆ–æ‰©å¤§æ£€ç´¢èŒƒå›´")
        else:
            st.info("å½“å‰æ£€ç´¢æ•ˆæœè‰¯å¥½")


with tab4:
    st.subheader("ç³»ç»Ÿé…ç½®")
    st.markdown(f"""
    - **åµŒå…¥æ¨¡å‹**: {EMBEDDING_MODEL}
    - **æ‘˜è¦æ¨¡å‹**: {SUMMARY_MODEL_ID}
    - **å·²å¤„ç†æ–‡æ¡£æ•°**: {len(st.session_state.processed_files)}
    - **å†…å­˜ä¸­çš„æ®µè½æ•°**: {len(st.session_state.vector_db.get()["documents"]) if st.session_state.vector_db else 0}
    """)

    st.subheader("ä½¿ç”¨æŒ‡å—")
    st.markdown(f"""
    1. **ä¸Šä¼ æ–‡æ¡£**: åœ¨å·¦ä¾§ä¾§æ ä¸Šä¼ PDF/DOCX/TXTæ ¼å¼çš„å­¦æœ¯æ–‡çŒ®
    2. **æ–‡æ¡£é€‰æ‹©**: åœ¨å·¦ä¾§æ¿€æ´»æ–‡æ¡£ä¸­é€‰æ‹©æ¿€æ´»æˆ–å–æ¶ˆéœ€è¦é‡ç‚¹åˆ†æçš„å­¦æœ¯æ–‡æ¡£
    3. **æ‘˜è¦æŸ¥çœ‹**: åœ¨å·¦ä¾§ä¾§æ æœ€ä¸‹æ–¹å¯é€šè¿‡å±•å¼€å¯¹åº”æ–‡çŒ®æŸ¥çœ‹ç›¸å…³æ–‡çŒ®çš„å…·ä½“æ‘˜è¦ä¿¡æ¯
    4. **æ™ºèƒ½é—®ç­”**: 
        - é—®é¢˜æé—®: åœ¨è¾“å…¥æ¡†ä¸­è¾“å…¥å…·ä½“é—®é¢˜å¹¶ç‚¹å‡»å‘é€
          ä¾‹å¦‚: è¯·å‘Šè¯‰æˆ‘è¯¥æ–‡çŒ®åœ¨å…·èº«æ™ºèƒ½é¢†åŸŸåšå‡ºçš„çªç ´æ˜¯ä»€ä¹ˆï¼Ÿ
        - è·¨æ–‡æ¡£åˆ†æ: æ”¯æŒå°†ä¸Šä¼ çš„å¤šç¯‡æ–‡çŒ®è¿›è¡Œè·¨æ–‡æ¡£åˆ†æï¼Œæä¾›æ›´ç»¼åˆçš„é—®ç­”ä½“éªŒ
          ä¾‹å¦‚: è¯·æ¯”è¾ƒè¿™ä¸‰ç¯‡æ–‡çŒ®çš„å®éªŒç»“æœ
    5. **æ£€ç´¢åˆ†æä¸ç»Ÿè®¡**:
        - æ–‡æ¡£ç»Ÿè®¡: æŸ¥çœ‹å„æ–‡æ¡£çš„æ®µè½åˆ†å¸ƒ 
        - æ¦‚å¿µè¿½è¸ª: å®šä½ç‰¹å®šæœ¯è¯­åœ¨æ‰€æœ‰æ–‡æ¡£ä¸­çš„ä½ç½®
    6. **æ£€ç´¢ä¼˜åŒ–**: æ ¹æ®æ£€ç´¢åˆ†æè°ƒæ•´æŸ¥è¯¢ç­–ç•¥  
    """)

    if st.button("æ¸…ç©ºæ–‡æ¡£"):
        init_session_state(True)
        st.rerun()

st.sidebar.divider()
st.sidebar.info(
    f"ç³»ç»ŸçŠ¶æ€: {len(st.session_state.active_docs)} ä¸ªæ¿€æ´»æ–‡æ¡£\n"
    "æŠ€æœ¯æ ˆ: Langchain + ChromaDB + HuggingFace"
)

